{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please implement all functions required in this notebook and write your observations and discussions as comments or markdown cells inside the notebook. Please submit the jupyter notebook along with all files that are required in the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with a file `data.csv` that contains the number of bicycles observed in several places in Ottawa during 2010 to 2019. The csv file has the following columns:\n",
    "\n",
    "| column name | description |\n",
    "|:-------------:|:-------------:|\n",
    "| location_name | the location where the counter was installed |\n",
    "| count      | number of bicycles passed by |\n",
    "| max temp | maximum temperature (Celsius) |\n",
    "| mean temp | average temperature (Celsius) |\n",
    "| min temp | minimum temperature (Celsius) |\n",
    "| snow on grnd (cm) | snow on ground |\n",
    "| total precip (mm) | total precipitation |\n",
    "| total rain (mm) | total rain |\n",
    "| total snow (cm) | total snow |\n",
    "| date | date of recording |\n",
    "\n",
    "\n",
    "This part requires you to perform **data engineering**, **classical machine learning** methods and **neural networks** methods to solve a multi-class classification problem. All code should be written in Python (within the provided Jupyter Notebook environment) with or without (but not restricted to) the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "#import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to use any other dependencies, please add them at the end of `requirements.txt` file. All code should be run on a CPU machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task objective:** given an input of `date, max temp, mean temp, min temp, snow on grnd (cm), total precip (mm), total rain (mm), total snow (cm)`, predict whether the tota\n",
    "l number of bicycles observed in a day is ***less than 2000***, ***in between 2000 and 10k*** or ***over 10k***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read through all the items below before you start the task, and write down as many comments as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please implement the function below which does the following two steps:\n",
    "\n",
    "1. First sum the counts at different locations in a day. In other words, you are creating a new `DataFrame` that contains the following columns:\n",
    "    - `date`\n",
    "    - `max temp`\n",
    "    - `mean temp`\n",
    "    - `min temp`\n",
    "    - `snow on grnd (cm)`\n",
    "    - `total precip (mm)`\n",
    "    - `total rain (mm)`\n",
    "    - `total snow (cm)`\n",
    "    - `total count`\n",
    "    \n",
    "\n",
    "2. Convert the numerical column `total count` into three categories: *less than 2000*, *2000 to 10000* and *over 10000*. Save the `processed_df` to `processed_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the numerical column total count into three\n",
    "#categories: less than 2000, 2000 to 10000 and over 10000. Save the processed_df to processed_data.csv.\n",
    "def divide_classes(x):\n",
    "    if x <  2000:\n",
    "        return \"less than 2000\"\n",
    "    elif x >= 2000 & x <= 10000:\n",
    "        return \"2000 to 10000\"    \n",
    "    elif x > 10000:\n",
    "        return \"Over 10000\"\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "def preprocessing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Pre-process a dataframe\n",
    "    \n",
    "    :param pd.DataFrame df: raw dataframe from data.csv\n",
    "    \n",
    "    :returns pd.DataFrame processed_df: processed dataframe\n",
    "    ''' \n",
    "    \n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    df1 = df.groupby(['date' , 'location_name' ], as_index=False)['count'].sum()\n",
    "\n",
    "    df1.columns = ['date', 'location_name', 'total count']\n",
    "    #del df['location_name']\n",
    "\n",
    "    processed_df = pd.merge(df1, df, how = 'left', on = ['date', 'location_name'])\n",
    "    print(processed_df.shape)\n",
    "    print(processed_df.head(5))\n",
    "\n",
    "    del processed_df['location_name']\n",
    "    del processed_df['count']\n",
    "\n",
    "    processed_df['class'] = processed_df['total count'].apply(lambda x : divide_classes(x))\n",
    "    print(processed_df.head())\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please implement the function below to do any data engineering as needed, split the dataset into train set (80%) and test set (20%). The final `DataFrame` should be ready for training/testing, in other words, one should be able to get `x, y` arrays using the following commands:\n",
    "\n",
    "```python\n",
    "x_train = train_df.drop(columns=['total count']).values\n",
    "y_train = train_df['total count'].values\n",
    "```\n",
    "\n",
    "Save `train_df`, `test_df` to `train.csv` and `test.csv` respectively.\n",
    "\n",
    "*hint: please consider what additional information can be extracted from the given columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def data_engineering(processed_df: pd.DataFrame) -> (pd.DataFrame,\n",
    "                                                     pd.DataFrame):\n",
    "    \n",
    "    '''\n",
    "    Perform data engineering on processed dataframe\n",
    "\n",
    "    :param pd.DataFrame processed_df: output of preprocess()\n",
    "\n",
    "    :returns pd.DataFrame train_df: training set of the engineered dataframe\n",
    "    :returns pd.DataFrame test_df: test set of the engineered dataframe\n",
    "    '''\n",
    "    \n",
    "    # drop the null values\n",
    "    print(processed_df.isnull().sum().tolist())\n",
    "    processed_df = processed_df.dropna(axis=1, how='all')\n",
    "    print(processed_df.isnull().sum().tolist())\n",
    "\n",
    "    processed_df['day'] = pd.DatetimeIndex(df['date']).day\n",
    "    processed_df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "    processed_df['year']= pd.DatetimeIndex(df['date']).year\n",
    "    del processed_df['date']\n",
    "    processed_df.head()\n",
    "\n",
    "    # train test split\n",
    "    train_df, test_df = train_test_split(processed_df, test_size=0.2)\n",
    "    \n",
    "    # save dataframe as csv\n",
    "    train_df.to_csv('train.csv', sep='\\t')\n",
    "    test_df.to_csv('test.csv', sep='\\t')\n",
    "    \n",
    "    return (train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Classical machine learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the function below which takes `train_df` and `test_df` as inputs, and outputs the trained classifier, accuracy on training set and test set, confusion matrix on test set. You can experiment with as many methods as you want, please only leave the one with best performance (you can leave the others in comments) so that the function only returns one trained classifier.\n",
    "\n",
    "Please answer the following questions as comments or in another markdown cell:\n",
    "\n",
    "1. What data engineering techniques did you apply?\n",
    "2. What's the best accuracy on test set did you achieve? Which classifier did you use to get the best accuracy?\n",
    "3. Which features are the most important for predicting counts of bicycles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/python_user/.pyenv/versions/3.7.2/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# ml models decistion tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# One of the Ensemle model -- Random Forset which is ensemple of decision trees\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "# import logistic regression, SVC  for ensembling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Distance based\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# packages for metric for evalaution of the models\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report , accuracy_score, confusion_matrix\n",
    "\n",
    "# to save the models so that we don't need to train the models every time we need to do prediction\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23028, 10)\n",
      "  location_name  count  max temp  mean temp  min temp  snow on grnd (cm)  \\\n",
      "0          ALEX      0     -14.0      -18.2     -22.3               27.0   \n",
      "1    ADAWE BIKE    968      29.5       23.8      18.0                0.0   \n",
      "2          COBY     10      -3.0       -9.3     -15.6                1.0   \n",
      "3          SOMO    122      14.0        8.8       3.5                0.0   \n",
      "4          OYNG    228      15.2        9.4       3.5                0.0   \n",
      "\n",
      "   total precip (mm)  total rain (mm)  total snow (cm)        date  \n",
      "0                0.3              0.0              0.3  2015-02-20  \n",
      "1                2.8              2.8              0.0  2016-08-28  \n",
      "2                7.0              0.0              9.5  2011-12-25  \n",
      "3               34.4             34.4              0.0  2017-05-01  \n",
      "4                0.0              0.0              0.0  2015-04-27  \n",
      "(23028, 11)\n",
      "         date location_name  total count  count  max temp  mean temp  \\\n",
      "0  2010-01-01          ALEX            0      0      -2.5       -4.1   \n",
      "1  2010-01-01          COBY            0      0      -2.5       -4.1   \n",
      "2  2010-01-01          ORPY            0      0      -2.5       -4.1   \n",
      "3  2010-01-02          ALEX            0      0     -11.9      -13.3   \n",
      "4  2010-01-02          ORPY            0      0     -11.9      -13.3   \n",
      "\n",
      "   min temp  snow on grnd (cm)  total precip (mm)  total rain (mm)  \\\n",
      "0      -5.7               18.0                5.0              0.0   \n",
      "1      -5.7               18.0                5.0              0.0   \n",
      "2      -5.7               18.0                5.0              0.0   \n",
      "3     -14.6               21.0                1.9              0.0   \n",
      "4     -14.6               21.0                1.9              0.0   \n",
      "\n",
      "   total snow (cm)  \n",
      "0              7.6  \n",
      "1              7.6  \n",
      "2              7.6  \n",
      "3              2.6  \n",
      "4              2.6  \n",
      "         date  total count  max temp  mean temp  min temp  snow on grnd (cm)  \\\n",
      "0  2010-01-01            0      -2.5       -4.1      -5.7               18.0   \n",
      "1  2010-01-01            0      -2.5       -4.1      -5.7               18.0   \n",
      "2  2010-01-01            0      -2.5       -4.1      -5.7               18.0   \n",
      "3  2010-01-02            0     -11.9      -13.3     -14.6               21.0   \n",
      "4  2010-01-02            0     -11.9      -13.3     -14.6               21.0   \n",
      "\n",
      "   total precip (mm)  total rain (mm)  total snow (cm)           class  \n",
      "0                5.0              0.0              7.6  less than 2000  \n",
      "1                5.0              0.0              7.6  less than 2000  \n",
      "2                5.0              0.0              7.6  less than 2000  \n",
      "3                1.9              0.0              2.6  less than 2000  \n",
      "4                1.9              0.0              2.6  less than 2000  \n",
      "[0, 0, 106, 106, 106, 120, 120, 120, 120, 0]\n",
      "[0, 0, 106, 106, 106, 120, 120, 120, 120, 0]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = data_engineering(preprocessing(df))\n",
    "\n",
    "x_train.to_csv(\"train\")\n",
    "x_test.to_csv(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 106, 106, 106, 120, 120, 120, 120, 0, 0, 0, 0]\n",
      "[0, 106, 106, 106, 120, 120, 120, 120, 0, 0, 0, 0]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\timeseriesanalysis\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3077\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3078\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-a8baca0c4cbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_engineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# trying different models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m models = [\n\u001b[0;32m      5\u001b[0m     \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-a627001de992>\u001b[0m in \u001b[0;36mdata_engineering\u001b[1;34m(processed_df)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprocessed_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatetimeIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprocessed_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatetimeIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdel\u001b[0m \u001b[0mprocessed_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mprocessed_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\timeseriesanalysis\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__delitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2741\u001b[0m             \u001b[1;31m# there was no match, this call should raise the appropriate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2742\u001b[0m             \u001b[1;31m# exception:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2743\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2745\u001b[0m         \u001b[1;31m# delete from the caches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\timeseriesanalysis\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mdelete\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   4172\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mselected\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnon\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4173\u001b[0m         \"\"\"\n\u001b[1;32m-> 4174\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4176\u001b[0m         \u001b[0mis_deleted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\timeseriesanalysis\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3078\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "#x_train = \n",
    "#y_train =\n",
    "#x_test = \n",
    "#y_test = \n",
    "\n",
    "\n",
    "# trying different models\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    KNeighborsClassifier(n_neighbors=5),\n",
    "    MultinomialNB()\n",
    "]\n",
    "\n",
    "# 5 cross validation\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, x_train, y_train, scoring='accuracy', cv=CV)\n",
    "    \n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=10)\n",
    "random_forest.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Accuracy = \")\n",
    "print(random_forest.score(x_test, y_test))\n",
    "print(\"\\n\")\n",
    "y_pred_rf = random_forest.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_ml(train_df: pd.DataFrame, test_df: pd.DataFrame) -> (\n",
    "    'classifier', 'accuracy', 'confusion matrix'):\n",
    "    '''\n",
    "    Use classical machine learning methods to predict total counts\n",
    "\n",
    "    :param pd.DataFrame train_df: training set dataframe\n",
    "    :param pd.DataFrame test_df: test set dataframe\n",
    "\n",
    "    :returns 'classifier': trained classifier\n",
    "    :returns 'accuracy': tuple of training accuracy and testing accuracy\n",
    "    :returns 'confusion matrix': confusion matrix on test set\n",
    "    '''\n",
    "    x_train = train_df.drop(columns=['total count']).values\n",
    "    y_train = train_df['total count'].values\n",
    "    x_test = test_df.drop(columns=['total count']).values\n",
    "    y_test = test_df['total count'].values\n",
    "\n",
    "    #clf = model.fit(x_train, y_train, ...)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ...\n",
    "    return (clf, (train_acc, test_acc), confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the function below which takes `train_df` and `test_df` as inputs, and outputs the trained model, accuracy on test set. You can experiment with as many structures as you want, please only leave the one with the best performance so that the function only returns one model. In addition to the model and test accuracy, please also produce two graphs in the notebook cell:\n",
    "\n",
    "- accuracy on train and validation data, `x-axis`: epochs, `y-axis`: accuracy\n",
    "- loss on train and validation data, `x-axis`: epochs, `y-axis`: loss\n",
    "\n",
    "Note validation data is not test set, it should be split from training set.\n",
    "\n",
    "Please answer the following questions as comments or in another markdown cell:\n",
    "\n",
    "1. How many epochs did you train? How did you decide when to stop training?\n",
    "2. Please briefly explain the model structure (layers, sizes) you choose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_ml(train_df: pd.DataFrame, test: pd.DataFrame) ->  ('model',\n",
    "                                                           'test_accuracy'):\n",
    "    '''\n",
    "    Use neural networks to predict total counts\n",
    "\n",
    "    :param pd.DataFrame train_df: training set dataframe\n",
    "    :param pd.DataFrame test_df: test set dataframe\n",
    "\n",
    "    :returns 'model': trained model\n",
    "    :returns 'test_accuracy': accuracy on test set\n",
    "    '''\n",
    "    x_train = train_df.drop(columns=['total count']).values\n",
    "    y_train = train_df['total count'].values\n",
    "    x_test = test_df.drop(columns=['total count']).values\n",
    "    y_test = test_df['total count'].values\n",
    "\n",
    "    mdl.fit(x_train, y_train, ...)\n",
    "    ...\n",
    "\n",
    "    return (mdl, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussions\n",
    "\n",
    "Please write your observations, comments regarding this dataset and problem, you can also tell us about what challenges you faced or what you have learnt during your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optional\n",
    "\n",
    "If you are to design a forecasting model using this dataset to predict the counts of bicycles (still three categories, not actual numbers) in the future, what changes will you make? Please state all factors that you consider relevant, there is no need to write code for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you submit\n",
    "\n",
    "Please use this checklist to make sure you submit all required files:\n",
    "\n",
    "- [ ] `part1_answers.txt`\n",
    "- [ ] `part2_answers.ipynb` (with code implementations and discussions)\n",
    "- [ ] `processed_data.csv`\n",
    "- [ ] `train.csv`\n",
    "- [ ] `test.csv`\n",
    "- [ ] (optional) `requirements.txt` if you used any additional package that was not listed before"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
